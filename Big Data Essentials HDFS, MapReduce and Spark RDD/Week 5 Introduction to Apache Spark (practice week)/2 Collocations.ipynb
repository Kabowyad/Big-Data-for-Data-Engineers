{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark assignment 2: Collocations\n",
    "\n",
    "As for the second part of the assignment, your task is to extract collocations: that is word combinations that occur together. For example, “high school” or “roman empire”.\n",
    "\n",
    "To find collocations, you will use NPMI (normalized pointwise mutual information) metric.\n",
    "\n",
    "PMI of two words, a & b, is defined as “PMI(a, b) = ln (P(ab) / (P(a) * P(b))”, where P(ab) is the probability of two words coming one after the other, and P(a) and P(b) are probabilities of words a & b respectively.\n",
    "\n",
    "You will estimate probabilities with occurrence counts, that is “P(a) = # of occurrences of word a / total number of words”, and “P(ab) = # of occurrences of words ‘a b’ / total number of word pairs”.\n",
    "\n",
    "To build an intuition behind the definition, see Reading material.\n",
    "\n",
    "Therefore, rare combinations of coupled words have large PMI.\n",
    "\n",
    "NPMI is computed as “NPMI(a, b) = PMI(a, b) / -ln P(ab)”. This normalizes the quantity to be within the range [-1; 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = unicode(line.rstrip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "    text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    return words\n",
    "\n",
    "wiki = sc.textFile(\"/data/wiki/en_articles_part/articles-part\", 16).map(parse_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out stopwords using the dictionary (/datasets/stop_words_en.txt ) (do not forget to convert words to the lowercase!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/datasets/stop_words_en.txt') as f1:\n",
    "    stop_words = set(f1.read().split())\n",
    "    \n",
    "def flt(words):\n",
    "    res = list()\n",
    "    for item in words:\n",
    "        if item not in stop_words:\n",
    "            res.append(item)\n",
    "    return res\n",
    "\n",
    "wiki_lower = wiki.map(lambda words: [x.lower() for x in words])\n",
    "wiki_filtered = wiki_lower.map(lambda words: flt(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all bigrams (that is, pairs of consequent words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(words):\n",
    "    result = list()\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        result.append((word + '_' + words[i + 1], 1))\n",
    "    \n",
    "    return result\n",
    "\n",
    "bigrams = wiki_filtered.flatMap(lambda item: make_bigrams(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave only bigrams with at least 500 occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baigrams_occurrence = bigrams.reduceByKey(lambda accum, n: accum + n)\n",
    "filtered_bigrams = baigrams_occurrence.filter(lambda (bigram, count): count >= 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total words and bigrams count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = wiki_filtered.flatMap(lambda words: [item for item in words]).count()\n",
    "total_bigrams = bigrams.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_occurrence_rdd = wiki_filtered.flatMap(lambda words: [(word, 1) for word in words])\\\n",
    "                                    .reduceByKey(lambda accum, n: accum + n)\n",
    "\n",
    "words_prob = dict()\n",
    "\n",
    "for (word, count) in words_occurrence_rdd.collect():\n",
    "    words_prob[word] = float(count) / total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_prob = filtered_bigrams.map(lambda (bigram, count): (bigram, float(count) / total_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute NPMI for every bigram (note: when computing probabilities, you need unpruned counts!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def calc_npmi((bigram, prob)):\n",
    "    [left_word, right_word] = bigram.split(\"_\", 1)\n",
    "    left_prob = words_prob[left_word]\n",
    "    right_prob = words_prob[right_word]\n",
    "    \n",
    "    pmi = log(prob / (left_prob * right_prob))\n",
    "    npmi = pmi / -log(prob)\n",
    "    \n",
    "    return (npmi, bigram)\n",
    "\n",
    "NPMI = bigrams_prob.map(lambda item: calc_npmi(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort word pairs by NPMI in the descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_NPMI = NPMI.sortByKey(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top 39 word pairs, with words delimited by the underscore “_”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los_angeles\n",
      "external_links\n",
      "united_states\n",
      "prime_minister\n",
      "san_francisco\n",
      "et_al\n",
      "new_york\n",
      "supreme_court\n",
      "19th_century\n",
      "20th_century\n",
      "references_external\n",
      "soviet_union\n",
      "air_force\n",
      "baseball_player\n",
      "university_press\n",
      "roman_catholic\n",
      "united_kingdom\n",
      "references_reading\n",
      "notes_references\n",
      "award_best\n",
      "north_america\n",
      "new_zealand\n",
      "civil_war\n",
      "catholic_church\n",
      "world_war\n",
      "war_ii\n",
      "south_africa\n",
      "took_place\n",
      "roman_empire\n",
      "united_nations\n",
      "american_singer-songwriter\n",
      "high_school\n",
      "american_actor\n",
      "american_actress\n",
      "american_baseball\n",
      "york_city\n",
      "american_football\n",
      "years_later\n",
      "north_american\n"
     ]
    }
   ],
   "source": [
    "for (npmi, bigram) in sorted_NPMI.take(39):\n",
    "    print bigram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
