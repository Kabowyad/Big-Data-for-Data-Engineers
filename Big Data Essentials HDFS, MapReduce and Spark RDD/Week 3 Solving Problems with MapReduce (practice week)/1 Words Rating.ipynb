{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Streaming assignment 1: Words Rating\n",
    "\n",
    "Create your own WordCount program and process Wikipedia dump. Use the second job to sort words by quantity in the reverse order (most popular first). Output format:\n",
    "\n",
    "word <tab> count\n",
    "\n",
    "The result is the 7th word by popularity and its quantity.\n",
    "\n",
    "__Hint__: it is possible to use exactly one reducer in the second job to obtain a totally ordered result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_count_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_count_mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    counts = Counter(words)\n",
    "    for word, count in counts.items():\n",
    "        print \"%s\\t%d\" % (word.lower(), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_count_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_count_reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "current_key = None\n",
    "word_sum = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    if current_key != key:\n",
    "        if current_key:\n",
    "            print \"%s\\t%d\" % (current_key, word_sum)\n",
    "        word_sum = 0\n",
    "        current_key = key\n",
    "    word_sum += count\n",
    "\n",
    "if current_key:\n",
    "    print \"%s\\t%d\" % (current_key, word_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_rating_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_rating_mapper.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, count = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    print \"%d\\t%s\" % (count, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_rating_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_rating_reducer.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        count, word = line.strip().split('\\t', 1)\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    print \"%s\\t%d\" % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\t126420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-rm: Not enough arguments: expected 1 but got 0\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-x] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "command [genericOptions] [commandOptions]\n",
      "\n",
      "Usage: hadoop fs [generic options] -rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...\n",
      "18/08/02 08:18:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/08/02 08:18:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/08/02 08:18:51 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/08/02 08:18:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/08/02 08:18:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1533143790063_0001\n",
      "18/08/02 08:18:51 INFO impl.YarnClientImpl: Submitted application application_1533143790063_0001\n",
      "18/08/02 08:18:51 INFO mapreduce.Job: The url to track the job: http://6b428e5a5e59:8088/proxy/application_1533143790063_0001/\n",
      "18/08/02 08:18:51 INFO mapreduce.Job: Running job: job_1533143790063_0001\n",
      "18/08/02 08:18:58 INFO mapreduce.Job: Job job_1533143790063_0001 running in uber mode : false\n",
      "18/08/02 08:18:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/08/02 08:19:15 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "18/08/02 08:19:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/08/02 08:19:24 INFO mapreduce.Job:  map 100% reduce 13%\n",
      "18/08/02 08:19:25 INFO mapreduce.Job:  map 100% reduce 38%\n",
      "18/08/02 08:19:27 INFO mapreduce.Job:  map 100% reduce 63%\n",
      "18/08/02 08:19:29 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "18/08/02 08:19:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/08/02 08:19:30 INFO mapreduce.Job: Job job_1533143790063_0001 completed successfully\n",
      "18/08/02 08:19:30 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7451389\n",
      "\t\tFILE: Number of bytes written=16303762\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=5370513\n",
      "\t\tHDFS: Number of read operations=30\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=8\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=34321\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21965\n",
      "\t\tTotal time spent by all map tasks (ms)=34321\n",
      "\t\tTotal time spent by all reduce tasks (ms)=21965\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=34321\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=21965\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35144704\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=22492160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=4166206\n",
      "\t\tMap output bytes=40841856\n",
      "\t\tMap output materialized bytes=7451437\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=4166206\n",
      "\t\tCombine output records=522054\n",
      "\t\tReduce input groups=427175\n",
      "\t\tReduce shuffle bytes=7451437\n",
      "\t\tReduce input records=522054\n",
      "\t\tReduce output records=427175\n",
      "\t\tSpilled Records=1044108\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=748\n",
      "\t\tCPU time spent (ms)=31020\n",
      "\t\tPhysical memory (bytes) snapshot=2186391552\n",
      "\t\tVirtual memory (bytes) snapshot=20180410368\n",
      "\t\tTotal committed heap usage (bytes)=1558183936\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5370513\n",
      "18/08/02 08:19:30 INFO streaming.StreamJob: Output directory: wordcount_result_1533197927357569\n",
      "rm: `wordrating_result_1533197970580360': No such file or directory\n",
      "18/08/02 08:19:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/08/02 08:19:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/08/02 08:19:34 INFO mapred.FileInputFormat: Total input files to process : 8\n",
      "18/08/02 08:19:34 INFO mapreduce.JobSubmitter: number of splits:8\n",
      "18/08/02 08:19:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1533143790063_0002\n",
      "18/08/02 08:19:34 INFO impl.YarnClientImpl: Submitted application application_1533143790063_0002\n",
      "18/08/02 08:19:34 INFO mapreduce.Job: The url to track the job: http://6b428e5a5e59:8088/proxy/application_1533143790063_0002/\n",
      "18/08/02 08:19:34 INFO mapreduce.Job: Running job: job_1533143790063_0002\n",
      "18/08/02 08:19:40 INFO mapreduce.Job: Job job_1533143790063_0002 running in uber mode : false\n",
      "18/08/02 08:19:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/08/02 08:19:47 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "18/08/02 08:19:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/08/02 08:19:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/08/02 08:19:54 INFO mapreduce.Job: Job job_1533143790063_0002 completed successfully\n",
      "18/08/02 08:19:54 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6486115\n",
      "\t\tFILE: Number of bytes written=14234944\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5371553\n",
      "\t\tHDFS: Number of bytes written=5358350\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25338\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4039\n",
      "\t\tTotal time spent by all map tasks (ms)=25338\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4039\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=25338\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4039\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=25946112\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4135936\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=427175\n",
      "\t\tMap output records=427175\n",
      "\t\tMap output bytes=5631751\n",
      "\t\tMap output materialized bytes=6486157\n",
      "\t\tInput split bytes=1040\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=378650\n",
      "\t\tReduce shuffle bytes=6486157\n",
      "\t\tReduce input records=427175\n",
      "\t\tReduce output records=427174\n",
      "\t\tSpilled Records=854350\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=1164\n",
      "\t\tCPU time spent (ms)=17960\n",
      "\t\tPhysical memory (bytes) snapshot=2528534528\n",
      "\t\tVirtual memory (bytes) snapshot=18108628992\n",
      "\t\tTotal committed heap usage (bytes)=1740111872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5370513\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5358350\n",
      "18/08/02 08:19:54 INFO streaming.StreamJob: Output directory: wordrating_result_1533197970580360\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "WORD_COUNT_OUT_DIR=\"wordcount_result_\"$(date +\"%s%6N\")\n",
    "NUM_REDUCERS=8\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Streaming wordCount\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files word_count_mapper.py,word_count_reducer.py \\\n",
    "    -mapper \"python word_count_mapper.py\" \\\n",
    "    -combiner \"python word_count_reducer.py\" \\\n",
    "    -reducer \"python word_count_reducer.py\" \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${WORD_COUNT_OUT_DIR} > /dev/null\n",
    "    \n",
    "WORD_RATING_OUT_DIR=\"wordrating_result_\"$(date +\"%s%6N\")\n",
    "NUM_REDUCERS=1\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${WORD_RATING_OUT_DIR} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Streaming wordRating\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D stream.map.output.field.separator=\\t \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k1,1nr \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files word_rating_mapper.py,word_rating_reducer.py \\\n",
    "    -mapper \"python word_rating_mapper.py\" \\\n",
    "    -reducer \"python word_rating_reducer.py\" \\\n",
    "    -input ${WORD_COUNT_OUT_DIR} \\\n",
    "    -output ${WORD_RATING_OUT_DIR} > /dev/null\n",
    "\n",
    "hdfs dfs -cat ${WORD_RATING_OUT_DIR}/part-00000 | head -7 | tail -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
